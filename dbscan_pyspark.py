# -*- coding: utf-8 -*-
"""DBSCAN_PySpark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i_IWWicyWzHQcb5D3IG2udWagv6FS2lm

# Installations
"""

!pip install pandas
!pip install pyspark

import os
import time
import random
import warnings
import threading 
import numpy as np
import scipy as sp
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from IPython.display import display
import math
import os
import json
import random
import sys
from itertools import groupby
import matplotlib.cm as cm
import pyspark
from pyspark.sql import SparkSession
import time
import warnings
import numpy as np

# For data generation:
from sklearn import cluster, datasets, mixture
from sklearn.neighbors import kneighbors_graph
from sklearn.preprocessing import StandardScaler
from itertools import cycle, islice

"""# PySpark activation"""

spark = SparkSession.builder.master("local[1]") \
                    .appName('SparkByExamples.com') \
                    .getOrCreate()

sc=spark.sparkContext

"""# DBSCAN on PySpark"""

def PySpark_DBSCAN(working_data,epsilon,min_num_points):

  def create_cell_neighbors(cell):
    '''This function is used to connect each cell to its neighbors, turning the "dense" cells to a graph'''
    return [(cell[0]-1,cell[1]-1),(cell[0]-1,cell[1]),(cell[0]-1,cell[1]+1),(cell[0],cell[1]+1),(cell[0]+1,cell[1]+1),(cell[0]+1,cell[1]),(cell[0]+1,cell[1]-1),(cell[0],cell[1]-1)]

  def reduce_result_to_graph(quadrant_list):
    '''This function turns the first MapReduce results from a list of cells to a graph'''
    edges = []
    full_cells = dict(quadrant_list).keys()
    for cell in full_cells:
      for neighbor in create_cell_neighbors(cell):
        if neighbor in full_cells:
          edges.append((cell,neighbor))
          edges.append((neighbor,cell))
    return edges


  def small_mapper(edge):
    '''This function is a mapper for the "Small Star Operation" outlined in the "Connected Components in MapReduce and Beyond" article'''
    if edge[0] <= edge[1]:
      return edge
    else:
      return (edge[1],edge[0])

  def connect_to_small(node,neighbors):
    '''This function is a reducer for the "Small Star Operation" outlined in the "Connected Components in MapReduce and Beyond" article'''
    nodes = [i[1] for i in neighbors]
    minimal = min([node]+nodes)
    new_edges = []
    for neighbor in nodes:
      new_edges.append((minimal,nighbor))


  def small_star_map(edge):
    '''This function is a second mapper for the "Small Star Operation" outlined in the "Connected Components in MapReduce and Beyond" article'''
    if edge[1] <= edge[0]:
      return edge
    elif edge[1] > edge[0]:
      return (edge[1],edge[0])
    else:
      print("error")

  def small_reductor(key_val_tuple):
    '''This function is a second reducer for the "Small Star Operation" outlined in the "Connected Components in MapReduce and Beyond" article'''
    nodes = [key_val_tuple[0]] + key_val_tuple[1]
    minimal = min(nodes)
    nodes.remove(minimal)
    return [(i,minimal) for i in nodes]


  def large_star_reduce(key_val_tuple):
    '''This function is the reducer for the "Large Star Operation" outlined in the "Connected Components in MapReduce and Beyond" article'''
    nodes = [key_val_tuple[0]] + key_val_tuple[1]
    minimal = min(nodes)
    nodes = list(filter(lambda nodes: nodes >= key_val_tuple[0], nodes))
    return [(i,minimal) for i in nodes]

  def round(graph):
    '''This function does one "round" of the "Alternating" algorithm from the article, doing small-star-MapRedue and the large-star-MapReduce'''
    first_large_star = sc.parallelize(graph).groupByKey().mapValues(list).map(lambda row: large_star_reduce(row)).collect()
    first_small_star = sc.parallelize([item for sublist in first_large_star for item in sublist]).map(lambda row: small_star_map(row)).groupByKey().mapValues(list).map(lambda row: small_reductor(row)).collect()
    flattened = [item for sublist in first_small_star for item in sublist]
    flattened += [reverser(i) for i in flattened]
    return list(set(filter(lambda flattened: flattened[0] != flattened[1], flattened)))

  def reverser(edge):
    '''Helper function for the large star mapping'''
    return (edge[1],edge[0])

  def alternating_stars(graph):
    '''This function does rounds of the "Alternating" algorithm from the article until convergence'''
    finished = False
    last_graph = graph
    while not finished:
      new_graph = round(last_graph)
      if set(new_graph) == set(last_graph):
        finished = True
      else:
        last_graph = new_graph
    return sc.parallelize(new_graph).map(lambda row: small_mapper(row)).groupBy(lambda x: x[0]).map(lambda x: (x[0], list(x[1]))).collect()

  def normalize(v):
    '''This function normalizes the data to go from 0 to 1, in both axes'''
    gap = v.max()-v.min()
    v1 = v/gap
    min = v1.min()
    v2 = v1 - np.array([min for i in v])
    return v2


  def mapfunc(point):
    '''This is the mapper for the first MapReduce, which turns points into cells'''
    x_cell = math.floor((point[0]-x_min)/epsilon)
    y_cell = math.floor((point[1]-y_min)/epsilon)
    return [(x_cell,y_cell),1]

  def reducefunc(key, values):
    counts = [x[1] for x in values]
    if sum(counts) >= min_num_points:
      return [key,sum(counts)]


  x=normalize(working_data[working_data.columns.values.tolist()[0]])
  y=normalize(working_data[working_data.columns.values.tolist()[1]])

  range_x = x.max()-x.min()
  range_y = y.max()-y.min()
  x_min = x.min()
  y_min = y.min()

  working_data=pd.DataFrame({'x': x, 'y': y}, columns=['x', 'y']).to_numpy()

  cells = sc.parallelize(working_data).map(lambda row: mapfunc(row)).reduceByKey(lambda x, y: x + y).collect()
  cells = list(set(filter(lambda cells: cells[1]>min_num_points, cells)))

  return alternating_stars(reduce_result_to_graph(cells))

"""# The Data"""

np.random.seed(0)

n_samples = 500
noisy_circles = datasets.make_circles(n_samples=n_samples, factor=0.5, noise=0.05)
noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05)
blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)
no_structure = np.random.rand(n_samples, 2), None

# Anisotropicly distributed data
random_state = 170
X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)
transformation = [[0.6, -0.6], [-0.4, 0.8]]
X_aniso = np.dot(X, transformation)
aniso = (X_aniso, y)

# blobs with varied variances
varied = datasets.make_blobs(
    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state
)

# ============
# Set up cluster parameters
# ============
plt.figure(figsize=(9 * 2 + 3, 13))
plt.subplots_adjust(
    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01
)

plot_num = 1

default_base = {
    "quantile": 0.3,
    "eps": 0.3,
    "damping": 0.9,
    "preference": -200,
    "n_neighbors": 3,
    "n_clusters": 3,
    "min_samples": 7,
    "xi": 0.05,
    "min_cluster_size": 0.1,
}

datasets_gen = [
    (
        noisy_circles,
        {
            "damping": 0.77,
            "preference": -240,
            "quantile": 0.2,
            "n_clusters": 2,
            "min_samples": 7,
            "xi": 0.08,
        },
    ),
    (
        noisy_moons,
        {
            "damping": 0.75,
            "preference": -220,
            "n_clusters": 2,
            "min_samples": 7,
            "xi": 0.1,
        },
    ),
    (
        varied,
        {
            "eps": 0.18,
            "n_neighbors": 2,
            "min_samples": 7,
            "xi": 0.01,
            "min_cluster_size": 0.2,
        },
    ),
    (
        aniso,
        {
            "eps": 0.15,
            "n_neighbors": 2,
            "min_samples": 7,
            "xi": 0.1,
            "min_cluster_size": 0.2,
        },
    ),
    (blobs, {"min_samples": 7, "xi": 0.1, "min_cluster_size": 0.2}),
    (no_structure, {}),
]

"""# Graphics"""

def color_quadrants(quad_list):
  counter = 0
  colors=cm.rainbow(np.linspace(0, 1, len(quad_list)))
  color_dict = {}
  for i in quad_list:
    color_dict[i[0]] = colors[counter]
    for j in i[1]:
      color_dict[j[1][0],j[1][1]]=colors[counter]
    counter += 1
  return color_dict

dot_colors = color_quadrants(PySpark_DBSCAN(pd.DataFrame(datasets_gen[0][0][0]),0.08,3))
for i in dot_colors:
  plt.scatter(i[0],i[1],color=dot_colors[i])

def draw_clusters(df,epsilon,min_num_points):
  working_data = df[[df.columns.values.tolist()[0],df.columns.values.tolist()[1] ]]
  def normalize(v):
    # This function normalizes the data to go from 0 to 1, in both axes
      gap = v.max()-v.min()
      v1 = v/gap
      min = v1.min()
      v2 = v1 - np.array([min for i in v])
      return v2
  x=normalize(working_data[working_data.columns.values.tolist()[0]])
  y=normalize(working_data[working_data.columns.values.tolist()[1]])

  range_x = x.max()-x.min()
  range_y = y.max()-y.min()
  x_min = x.min()
  y_min = y.min()
  working_data=pd.DataFrame({'x': x, 'y': y}, columns=['x', 'y'])
  def mapfunc(point):
      '''This is the mapper for the first MapReduce, which turns points into cells'''
      x_cell = math.floor((point[0]-x_min)/epsilon)
      y_cell = math.floor((point[1]-y_min)/epsilon)
      return [(x_cell,y_cell),1]
  dot_colors = color_quadrants(PySpark_DBSCAN(working_data,epsilon,min_num_points))
  for i in working_data.to_numpy():
    cell = mapfunc(i)[0]
    if cell in dot_colors:
      plt.scatter(i[0],i[1],color=dot_colors[cell])
    else:
      plt.scatter(i[0],i[1],color="b")

draw_clusters(pd.DataFrame(datasets_gen[0][0][0]),0.08,3)

"""# Linear comparison"""

class Graph_structure:

   def __init__(self, V):
      self.V = V
      self.adj = [[] for i in range(V)]

   def DFS_Utility(self, temp, v, visited):

      visited[v] = True

      temp.append(v)

      for i in self.adj[v]:
         if visited[i] == False:

            temp = self.DFS_Utility(temp, i, visited)
      return temp

   def add_edge(self, v, w):
      self.adj[v].append(w)
      self.adj[w].append(v)

   def connected_components(self):
      visited = []
      connected_comp = []
      for i in range(self.V):
         visited.append(False)
      for v in range(self.V):
         if visited[v] == False:
            temp = []
            connected_comp.append(self.DFS_Utility(temp, v, visited))
      return connected_comp
def cells_alias(quadrant_list):
  alias = {}
  reverse_alias = {}
  counter = 0
  for cell in quadrant_list:
    alias[cell] = counter
    reverse_alias[counter] = cell
    counter += 1
  return alias,reverse_alias

def Linear_DBSCAN(working_data,epsilon,min_num_points):

  def create_cell_neighbors(cell):
    # This function is used to connect each cell to its neighbors, turning the "dense" cells to a graph
    return [(cell[0]-1,cell[1]-1),(cell[0]-1,cell[1]),(cell[0]-1,cell[1]+1),(cell[0],cell[1]+1),(cell[0]+1,cell[1]+1),(cell[0]+1,cell[1]),(cell[0]+1,cell[1]-1),(cell[0],cell[1]-1)]

  def reduce_result_to_graph(quadrant_list):
     # This function turns the first MapReduce results from a list of cells to a graph
    edges = []
    full_cells = dict(quadrant_list).keys()
    for cell in full_cells:
      for neighbor in create_cell_neighbors(cell):
        if neighbor in full_cells:
          edges.append((cell,neighbor))
          edges.append((neighbor,cell))
    return edges


  def small_mapper(edge):
    # This function is a mapper for the "Small Star Operation" outlined in the "Connected Components in MapReduce and Beyond" article
    if edge[0] <= edge[1]:
      return edge
    else:
      return (edge[1],edge[0])

  def connect_to_small(node,neighbors):
    # This function is a reducer for the "Small Star Operation" outlined in the "Connected Components in MapReduce and Beyond" article
    nodes = [i[1] for i in neighbors]
    minimal = min([node]+nodes)
    new_edges = []
    for neighbor in nodes:
      new_edges.append((minimal,nighbor))


  def small_star_map(edge):
    # This function is a second mapper for the "Small Star Operation" outlined in the "Connected Components in MapReduce and Beyond" article
    if edge[1] <= edge[0]:
      return edge
    elif edge[1] > edge[0]:
      return (edge[1],edge[0])
    else:
      print("error")

  def small_reductor(weird_tuple):
    # This function is a second reducer for the "Small Star Operation" outlined in the "Connected Components in MapReduce and Beyond" article
    nodes = [weird_tuple[0]] + weird_tuple[1]
    minimal = min(nodes)
    nodes.remove(minimal)
    return [(i,minimal) for i in nodes]


  def large_star_reduce(weird_tuple):
    # This function is the reducer for the "Large Star Operation" outlined in the "Connected Components in MapReduce and Beyond" article
    nodes = [weird_tuple[0]] + weird_tuple[1]
    minimal = min(nodes)
    nodes = list(filter(lambda nodes: nodes >= weird_tuple[0], nodes))
    return [(i,minimal) for i in nodes]

  def round(graph):
    # This function does one "round" of the "Alternating" algorithm from the article, doing small-star-MapRedue and the large-star-MapReduce
    first_large_star = sc.parallelize(graph).groupByKey().mapValues(list).map(lambda row: large_star_reduce(row)).collect()
    first_small_star = sc.parallelize([item for sublist in first_large_star for item in sublist]).map(lambda row: small_star_map(row)).groupByKey().mapValues(list).map(lambda row: small_reductor(row)).collect()
    flattened = [item for sublist in first_small_star for item in sublist]
    flattened += [reverser(i) for i in flattened]
    return list(set(filter(lambda flattened: flattened[0] != flattened[1], flattened)))

  def reverser(edge):
    # Helper function for the large star mapping
    return (edge[1],edge[0])

  def whole_round(graph):
    # This function does rounds of the "Alternating" algorithm from the article until convergence
    finished = False
    last_graph = graph
    while not finished:
      new_graph = round(last_graph)
      if set(new_graph) == set(last_graph):
        finished = True
      else:
        last_graph = new_graph
    return sc.parallelize(new_graph).map(lambda row: small_mapper(row)).groupBy(lambda x: x[0]).map(lambda x: (x[0], list(x[1]))).collect()

  def normalize(v):
    # This function normalizes the data to go from 0 to 1, in both axes
      gap = v.max()-v.min()
      v1 = v/gap
      min = v1.min()
      v2 = v1 - np.array([min for i in v])
      return v2


  def mapfunc(point):
    # This is the mapper for the first MapReduce, which turns points into cells
      x_cell = math.floor((point[0]-x_min)/epsilon)
      y_cell = math.floor((point[1]-y_min)/epsilon)
      return [(x_cell,y_cell),1]

  def reducefunc(key, values):
      counts = [x[1] for x in values]
      if sum(counts) >= min_num_points:
        return [key,sum(counts)]


  x=normalize(working_data[working_data.columns.values.tolist()[0]])
  y=normalize(working_data[working_data.columns.values.tolist()[1]])

  range_x = x.max()-x.min()
  range_y = y.max()-y.min()
  x_min = x.min()
  y_min = y.min()

  working_data=pd.DataFrame({'x': x, 'y': y}, columns=['x', 'y']).to_numpy()

  cells = {}
  for point in working_data:
    cell = mapfunc(point)[0]
    if cell in cells:
      cells[cell] += 1
    else:
      cells[cell] = 1
  
  cells = {k: v for k, v in cells.items() if v > 1}
  edges = []
  for cell in cells.keys():
      for neighbor in create_cell_neighbors(cell):
        if neighbor in cells:
          edges.append((cell,neighbor))
          edges.append((neighbor,cell))
  
  graph = Graph_structure(len(cells))
  cell_alises,cell_reverse_alises = cells_alias(cells)
  for edge in edges:
    graph.add_edge(cell_alises[edge[0]],cell_alises[edge[1]])
  conn_comp = graph.connected_components()
  new_conn_comp = []
  for comp in conn_comp:
    new_conn_comp.append((cell_reverse_alises[comp[0]],comp))
  newer_conn_comp = []
  for comp in new_conn_comp:
    internal_conn_comp = []
    for node in comp[1]:
      internal_conn_comp.append((comp[0],cell_reverse_alises[node]))
    newer_conn_comp.append((comp[0],internal_conn_comp))
  return newer_conn_comp

# Commented out IPython magic to ensure Python compatibility.
# %time spark_res = PySpark_DBSCAN(pd.DataFrame(datasets_gen[0][0][0]),0.08,3)
# %time lin_res = Linear_DBSCAN(pd.DataFrame(datasets_gen[0][0][0]),0.08,3)